<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Intro  The tutorial from the Lawrence Livermore National Laboratory And the one from ETH  Parallel computer taxonomy by Flynn  SISD: Base case of a single instruction stream and single data stream SIMD  Single instruction: all processing units execute the same instruction at any given clock cycle Multiple data: Each processing unit can operate on a different data element   MISD: multiband filter, code cracking MIMD: every computer now  Jargons  A node = a computer in a box execution units hardware threads  Speedup and scalability Amdahl&rsquo;s law  Strong scaling If a fraction $p$ of your code execution time can be parallelized, then ideally the best speedup from parallelize it is $\frac{1}{1-p}$."><title>Parallel computing</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://yifanyin.github.io//icon.png><link href=https://yifanyin.github.io/styles.8562e12869e583b14034359663c5f30d.min.css rel=stylesheet><link href=https://yifanyin.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://yifanyin.github.io/js/darkmode.e14499789948bbe72d1ea298a44bff5f.min.js></script>
<script src=https://yifanyin.github.io/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://yifanyin.github.io/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://yifanyin.github.io/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://yifanyin.github.io/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://yifanyin.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://yifanyin.github.io/",fetchData=Promise.all([fetch("https://yifanyin.github.io/indices/linkIndex.a21fced4e32b642bf0c364f076ef6355.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://yifanyin.github.io/indices/contentIndex.fe0685794ae0d906be6a1b1406915266.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://yifanyin.github.io",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://yifanyin.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:5,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:1,scale:1}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:.8})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/yifanyin.github.io\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://yifanyin.github.io/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://yifanyin.github.io/>üèî Yifan Yin üßëüèª‚Äçüíª</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Parallel computing</h1><p class=meta>Last updated
Aug 29, 2022</p><ul class=tags><li><a href=https://yifanyin.github.io/tags/computing/>Computing</a></li><li><a href=https://yifanyin.github.io/tags/coding/>Coding</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#intro>Intro</a><ol><li><a href=#parallel-computer-taxonomy-by-flynn>Parallel computer taxonomy by Flynn</a></li><li><a href=#jargons>Jargons</a></li></ol></li><li><a href=#speedup-and-scalability>Speedup and scalability</a><ol><li><a href=#amdahls-law>Amdahl&rsquo;s law</a></li><li><a href=#gustavson>Gustavson</a></li></ol></li><li><a href=#parallel-memory-architectures>Parallel memory architectures</a><ol><li><a href=#shared-memory>Shared memory</a><ol><li><a href=#uniform-memory-access-uma>Uniform Memory Access (UMA)</a></li><li><a href=#non-uniform-memory-access-numa>Non-Uniform Memory Access (NUMA)</a></li></ol></li><li><a href=#distributed-memory>Distributed memory</a><ol><li><a href=#advantages>Advantages</a></li><li><a href=#disadvantages>Disadvantages</a></li></ol></li></ol></li></ol></nav></details></aside><a href=#intro><h1 id=intro><span class=hanchor arialabel=Anchor># </span>Intro</h1></a><ul><li>The
<a href=https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial rel=noopener>tutorial</a> from the Lawrence Livermore National Laboratory</li><li>And
<a href=https://scicomp.ethz.ch/wiki/Parallel_computing rel=noopener>the one</a> from ETH</li></ul><a href=#parallel-computer-taxonomy-by-flynn><h2 id=parallel-computer-taxonomy-by-flynn><span class=hanchor arialabel=Anchor># </span>Parallel computer taxonomy by Flynn</h2></a><ul><li>SISD: Base case of a single instruction stream and single data stream</li><li>SIMD<ul><li>Single instruction: all processing units execute the same instruction at any given clock cycle</li><li>Multiple data: Each processing unit can operate on a different data element</li></ul></li><li>MISD: multiband filter, code cracking</li><li>MIMD: every computer now</li></ul><a href=#jargons><h2 id=jargons><span class=hanchor arialabel=Anchor># </span>Jargons</h2></a><ul><li>A node = a computer in a box</li><li>execution units</li><li>hardware threads</li></ul><a href=#speedup-and-scalability><h1 id=speedup-and-scalability><span class=hanchor arialabel=Anchor># </span>Speedup and scalability</h1></a><a href=#amdahls-law><h2 id=amdahls-law><span class=hanchor arialabel=Anchor># </span>Amdahl&rsquo;s law</h2></a><ul><li>Strong scaling</li><li>If a fraction $p$ of your code execution time can be parallelized, then ideally the best speedup from parallelize it is $\frac{1}{1-p}$.<ul><li>So if 50% of the time the code is doing serial work, parallelizing it makes the maximum speedup as 2, or two times faster.</li></ul></li><li>However this is what you can achieve with $s$-part parallelization if efficiency grows linearly with process number.$$S_\text{latency}=\frac{1}{1-p+\frac{p}{s}}$$<ul><li>Then the above example with 4 cores gives $S_\text{max}$ = 1/(1-0.5+0.5/4)=1.6, or 1.6 time speedup</li></ul></li><li>And there is the cost of programmer time</li><li>There is overhead in parallel code execution</li></ul><a href=#gustavson><h2 id=gustavson><span class=hanchor arialabel=Anchor># </span>Gustavson</h2></a><ul><li>Weak scaling</li><li>Problem size per core stay the same</li></ul><a href=#parallel-memory-architectures><h1 id=parallel-memory-architectures><span class=hanchor arialabel=Anchor># </span>Parallel memory architectures</h1></a><a href=#shared-memory><h2 id=shared-memory><span class=hanchor arialabel=Anchor># </span>Shared memory</h2></a><p>Describes a computer architecture where all processors have direct access to common physical memory. In a programming sense, it describes a model where parallel tasks all have the same &ldquo;picture&rdquo; of memory and can directly address and access the same logical memory locations regardless of where the physical memory actually exists.</p><a href=#uniform-memory-access-uma><h3 id=uniform-memory-access-uma><span class=hanchor arialabel=Anchor># </span>Uniform Memory Access (UMA)</h3></a><ul><li><strong>Symmetric Multiprocessor (SMP)</strong>¬†machines with Identical processors and equal access and access times to memory</li><li>Cache coherent</li></ul><a href=#non-uniform-memory-access-numa><h3 id=non-uniform-memory-access-numa><span class=hanchor arialabel=Anchor># </span>Non-Uniform Memory Access (NUMA)</h3></a><ul><li>Often made by physically linking two or more SMPs</li><li>One SMP can directly access memory of another SMP</li><li>When more SMPs exist, the geometrically increasing linkage becomes harder to manage.</li></ul><a href=#distributed-memory><h2 id=distributed-memory><span class=hanchor arialabel=Anchor># </span>Distributed memory</h2></a><ul><li>No global address space known to individual processors</li><li>No cache coherency</li><li>When a processor needs access to data in another processor, it is usually the task of the programmer to explicitly define how and when data is communicated. Synchronization between tasks is likewise the programmer&rsquo;s responsibility.</li></ul><a href=#advantages><h3 id=advantages><span class=hanchor arialabel=Anchor># </span>Advantages</h3></a><ul><li>Memory is scalable with the number of processors. Increase the number of processors and the size of memory increases proportionately.</li><li>Each processor can rapidly access its own memory without interference and without the overhead incurred with trying to maintain global cache coherency.</li><li>Cost effectiveness: can use commodity, off-the-shelf processors and networking.</li></ul><a href=#disadvantages><h3 id=disadvantages><span class=hanchor arialabel=Anchor># </span>Disadvantages</h3></a><ul><li>The programmer is responsible for many of the details associated with data communication between processors.</li><li>It may be difficult to map existing data structures, based on global memory, to this memory organization.</li><li>Non-uniform memory access times: data residing on a remote node takes longer to access than node local data.</li></ul><p>Most of the clusters today are combinations of shared-memory machines networked together as a distributed memory system. Very convenient to build but hard to program.</p><p>OpenMP and MPI</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://yifanyin.github.io/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Yifan Yin using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://yifanyin.github.io/>Home</a></li><li><a href=https://yifanyin.github.io/notes/>All Notes</a></li><li><a href=https://yifanyin.github.io/tags/>All Tags</a></li><li><a href=https://twitter.com/quakeyifan>Twitter</a></li><li><a href=https://www.linkedin.com/in/quakeyifan/>LinkedIn</a></li></ul></footer></div></div></body></html>